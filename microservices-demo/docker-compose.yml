# docker-compose.yml

networks:
  microservices:
    driver: bridge

services:
  # MongoDB for User Service
  user-mongo:
    image: mongo:latest
    container_name: user-mongo
    restart: unless-stopped
    environment:
      MONGO_INITDB_DATABASE: user_db
    volumes:
      - user_mongo_data:/data/db
      - ./mongo/user-init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks: [microservices]
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/user_db --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # MongoDB for Todo Service
  todo-mongo:
    image: mongo:latest
    container_name: todo-mongo
    restart: unless-stopped
    environment:
      MONGO_INITDB_DATABASE: todos
    volumes:
      - todo_mongo_data:/data/db
      - ./mongo/todo-init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks: [microservices]
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/todos --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # RabbitMQ Message Broker
  rabbitmq:
    image: rabbitmq:3.12-management
    container_name: rabbitmq
    restart: unless-stopped
    networks: [microservices]
    ports:
      - "5672:5672" # AMQP protocol
      - "15672:15672" # Management UI
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 30s
      timeout: 10s
      retries: 3

  # Kong API Gateway
  kong:
    image: kong:latest
    container_name: kong-gateway
    restart: unless-stopped
    networks: [microservices]
    ports:
      - "8000:8000" # Proxy
      - "8001:8001" # Admin API
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
      KONG_PROXY_LISTEN: "0.0.0.0:8000"
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
      KONG_LOG_LEVEL: info
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
    volumes:
      - ./kong/kong.yml:/etc/kong/kong.yml:ro
    depends_on:
      - user-service
      - todo-service
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # User Service (NestJS + TypeScript)
  user-service:
    build:
      context: ./services/user-service
      dockerfile: Dockerfile
    container_name: user-service
    restart: unless-stopped
    ports:
      - "3001:3001" # HTTP & GraphQL
      - "50051:50051" # gRPC
    environment:
      - MONGODB_URL=mongodb://user-mongo:27017/user_db
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - RABBITMQ_EXCHANGE=user_exchange
      - NODE_ENV=production
    depends_on:
      user-mongo:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks: [microservices]

  # Todo Service (FastAPI + Python)
  todo-service:
    build:
      context: ./services/todo-service
      dockerfile: Dockerfile
    container_name: todo-service
    restart: unless-stopped
    ports:
      - "8002:8000" # Changed to 8002 to avoid conflict with Kong Admin API (8001)
      - "50052:50052" # gRPC
    environment:
      - MONGODB_URL=mongodb://todo-mongo:27017
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - PYTHONPATH=/app
    depends_on:
      todo-mongo:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks: [microservices]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend (Next.js)
  frontend:
    build:
      context: ./services/frontend
      dockerfile: Dockerfile
    container_name: frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_GATEWAY_URL=http://localhost:8000
      - NEXT_PUBLIC_TODO_SERVICE_URL=http://localhost:8002
      - NEXT_PUBLIC_USER_SERVICE_URL=http://localhost:3001
    depends_on:
      - kong
      - user-service
      - todo-service
    networks: [microservices]

  # Observability Stack
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.7
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - cluster.name=microservices-cluster
      - node.name=elasticsearch-node
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9300:9300"
    networks: [microservices]
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test:
        ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  logstash:
    image: docker.elastic.co/logstash/logstash:8.17.7
    container_name: logstash
    user: root
    ports:
      - "5044:5044" # Beats input
      - "5000:5000" # TCP input
      - "9600:9600" # HTTP API
    environment:
      - "LS_JAVA_OPTS=-Xmx1g -Xms1g"
      - "XPACK_MONITORING_ENABLED=false"
      - "LOG_LEVEL=info"
    volumes:
      - ./observability/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./observability/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:rw
      - ./observability/logstash/config/jvm.options:/usr/share/logstash/config/jvm.options:ro
      - ./observability/logstash/config/log4j2.properties:/usr/share/logstash/config/log4j2.properties:ro
      - ./observability/logstash/templates:/usr/share/logstash/templates:ro
      - logstash_data:/usr/share/logstash/data
      - logstash_logs:/usr/share/logstash/logs
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks: [microservices]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.17.7
    container_name: filebeat
    user: root
    volumes:
      - ./observability/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat_data:/usr/share/filebeat/data
    environment:
      - ELASTICSEARCH_HOST=elasticsearch:9200
      - LOGSTASH_HOST=logstash:5044
    depends_on:
      logstash:
        condition: service_healthy
    networks: [microservices]
    command: ["--strict.perms=false"]

  kibana:
    image: docker.elastic.co/kibana/kibana:8.17.7
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks: [microservices]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Add Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=200h"
      - "--web.enable-lifecycle"
    networks: [microservices]
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9090/",
        ]
      interval: 30s
      timeout: 5s
      retries: 3

  # Add Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3002:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      - GF_RENDERING_SERVER_URL=http://renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./observability/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - elasticsearch
    networks: [microservices]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # Add Zipkin
  zipkin:
    image: openzipkin/zipkin:latest
    container_name: zipkin
    ports:
      - "9411:9411"
    environment:
      - STORAGE_TYPE=mem
    networks: [microservices]
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9411/health",
        ]
      interval: 30s
      timeout: 5s
      retries: 3

volumes:
  user_mongo_data:
  todo_mongo_data:
  rabbitmq_data:
  elasticsearch_data:
  prometheus_data:
  grafana_data:
  logstash_data:
  logstash_logs:
  filebeat_data:
